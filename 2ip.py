#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#pip install beautifulsoup4
"""
–ü–∞—Ä—Å–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ mail.ru –¥–ª—è WSL
–£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip3 install requests beautifulsoup4 lxml
"""


import requests
from bs4 import BeautifulSoup
import sys
import subprocess
import os


def check_wsl_environment():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º—ã –≤ WSL –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ"""
    print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–µ–¥—É WSL...")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º—ã –≤ Linux (WSL)
    if not sys.platform.startswith('linux'):
        print("‚ö†Ô∏è  –≠—Ç–æ—Ç –∫–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è WSL/Linux")
        print("   –ù–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤ –¥—Ä—É–≥–∏—Ö —Å—Ä–µ–¥–∞—Ö")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É
    try:
        requests.get('https://google.com', timeout=5)
        print("‚úì –ò–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    except:
        print("‚úó –ù–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")
        return False

    return True


def install_dependencies():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è WSL"""
    print("\nüì¶ –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...")

    dependencies = {
        'requests': 'requests',
        'bs4': 'beautifulsoup4',
        'lxml': 'lxml'
    }

    missing_deps = []

    for import_name, package_name in dependencies.items():
        try:
            if import_name == 'bs4':
                __import__('bs4')
            else:
                __import__(import_name)
            print(f"‚úì {package_name} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except ImportError:
            print(f"‚úó {package_name} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            missing_deps.append(package_name)

    if missing_deps:
        print(f"\n‚è≥ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...")
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ pip3
            cmd = [sys.executable, "-m", "pip", "install"] + missing_deps
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

            if result.returncode == 0:
                print("‚úì –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
                return True
            else:
                print(f"‚úó –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {result.stderr}")
                return False

        except subprocess.TimeoutExpired:
            print("‚úó –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
            return False
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞: {e}")
            return False
    else:
        print("‚úì –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
        return True


def get_mail_ru_titles():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å mail.ru"""
    try:
        print("\nüåê –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ mail.ru...")

        # URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        url = "https://mail.ru"
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code != 200:
            print(f"‚úó –û—à–∏–±–∫–∞ HTTP: {response.status_code}")
            return

        print("‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        # –ü–∞—Ä—Å–∏–º HTML —Å lxml parser (–±—ã—Å—Ç—Ä–µ–µ –∏ –Ω–∞–¥–µ–∂–Ω–µ–µ)
        soup = BeautifulSoup(response.content, 'lxml')

        print("üîç –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π...")

        # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        titles = []

        # 1. –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∫–ª–∞—Å—Å–∞–º mail.ru
        selectors = [
            '.news__list__item__link',  # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            '.js-topnews__item',  # –¢–æ–ø –Ω–æ–≤–æ—Å—Ç–∏
            '.ph__project__news__item__link',  # –ù–æ–≤–æ—Å—Ç–∏ –≤ —Ö–µ–¥–µ—Ä–µ
            '[class*="news-tabs__item"]',  # –ù–æ–≤–æ—Å—Ç–∏ –≤ —Ç–∞–±–∞—Ö
            '[class*="title"] a',  # –°—Å—ã–ª–∫–∏ —Å –∫–ª–∞—Å—Å–æ–º title
            '.svelte-1jw2u6k',  # –ù–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã Svelte
            '[data-qa*="news"]',  # –ü–æ data-qa –∞—Ç—Ä–∏–±—É—Ç–∞–º
            '[data-testid*="news"]',  # –ü–æ data-testid –∞—Ç—Ä–∏–±—É—Ç–∞–º
        ]

        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if text and len(text) > 20 and text not in titles:
                    titles.append(text)

        # 2. –ü–æ–∏—Å–∫ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º h1-h4
        for i in range(1, 5):
            headers = soup.find_all(f'h{i}')
            for header in headers:
                text = header.get_text(strip=True)
                if text and len(text) > 15 and text not in titles:
                    titles.append(text)

        # 3. –ü–æ–∏—Å–∫ –ø–æ –∞—Ç—Ä–∏–±—É—Ç–∞–º, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
        news_elements = soup.find_all(attrs={
            'data-module': True,
            'data-newsid': True,
            'data-type': 'news'
        })

        for elem in news_elements:
            text = elem.get_text(strip=True)
            if text and len(text) > 15 and text not in titles:
                titles.append(text)

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if titles:
            print(f"\nüì∞ –ù–ê–ô–î–ï–ù–û {len(titles)} –ó–ê–ì–û–õ–û–í–ö–û–í:\n")
            print("=" * 80)

            for i, title in enumerate(titles[:25], 1):
                print(f"{i:2d}. {title}")

            print("=" * 80)
            print(f"–ü–æ–∫–∞–∑–∞–Ω–æ {min(25, len(titles))} –∏–∑ {len(titles)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            save_to_file(titles)

        else:
            print("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏")
            debug_page(soup)

    except requests.exceptions.RequestException as e:
        print(f"‚úó –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
    except Exception as e:
        print(f"‚úó –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")


def save_to_file(titles):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ–∞–π–ª"""
    try:
        with open('mail_ru_titles.txt', 'w', encoding='utf-8') as f:
            f.write("–ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å mail.ru\n")
            f.write("=" * 50 + "\n\n")
            for i, title in enumerate(titles, 1):
                f.write(f"{i}. {title}\n")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: mail_ru_titles.txt")
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


def debug_page(soup):
    """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    print("\nüêõ –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
    print("–ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã...")

    # –ò—â–µ–º –≤—Å–µ –∫–ª–∞—Å—Å—ã —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ 'news'
    news_classes = set()
    for element in soup.find_all(class_=True):
        for class_name in element['class']:
            if 'news' in class_name.lower():
                news_classes.add(class_name)

    if news_classes:
        print("–ù–∞–π–¥–µ–Ω—ã –∫–ª–∞—Å—Å—ã —Å 'news':", list(news_classes)[:10])

    # –ò—â–µ–º –≤—Å–µ data-–∞—Ç—Ä–∏–±—É—Ç—ã
    data_attrs = set()
    for element in soup.find_all(attrs=True):
        for attr in element.attrs:
            if attr.startswith('data-') and any(x in attr for x in ['news', 'item', 'link']):
                data_attrs.add(attr)

    if data_attrs:
        print("–ù–∞–π–¥–µ–Ω—ã data-–∞—Ç—Ä–∏–±—É—Ç—ã:", list(data_attrs)[:10])


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ü–∞—Ä—Å–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ mail.ru –¥–ª—è WSL")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–µ–¥—É
    if not check_wsl_environment():
        return

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    if not install_dependencies():
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é:")
        print("sudo apt update && sudo apt install python3-pip")
        print("pip3 install requests beautifulsoup4 lxml")
        return

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
    get_mail_ru_titles()

    print("\n" + "=" * 50)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ! –î–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:")
    print(f"   python3 {os.path.basename(__file__)}")


if __name__ == "__main__":
    main()
